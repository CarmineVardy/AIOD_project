Tra i dataset del progetto c'è un dataset che è sull'epatocarcinoma, però è estremamente sbilanciato. Sono tre classi, di cui la classe minoritaria ha all'attivo 10 pazienti. Sotto questo aspetto potrebbe essere una challenge Poi invece c'è un dataset che abbiamo già dato l'anno scorso, che è sui campioni di congenital heart disease, che è quello che vi avevo accennato la volta precedente. Su questo aspetto ci sono due datasets sostanzialmente che noi abbiamo, uno è sulle donne non incinte, che è stata già pubblicato e anche quello sull'epatocarcinoma è stato già complicato. Quindi avete una guida per poter confrontarvi eccetera. Poi se siete coraggiosi questo studio è andato avanti, io vi posso anche presentare i miei risultati, però purtroppo siccome la controparte è un po' lenta nel produrre il paper, è un anno che il paper è fermo. Però sostanzialmente la parte analitica è la stessa del primo paper. Il primo lavoro è su donne non incinte, il secondo dataset è su donne incinte. Quindi se vi sentite che non vi spaventa troppo, potremmo lavorare su quello delle donne non incinte. Io comunque vi faccio vedere i miei risultati. Poi per la parte di materiale e metodo è lo stesso del primo lavoro. E quindi però avete un dataset con un po' meno campioni del lavoro sulle donne non incinte perché sono un po' meno. Ci sono queste tre opzioni. Quindi primo problema, epatocarcinoma, dataset estremamente sbilanciato, tre classi, ma estremamente sbilanciato. Secondo, macro problematica, congenital heart disease. Ci sono due dataset, uno un po' più corposo, sulle donne non incinte, ma già affrontate in precedenza. Che significa? che se fate un'analisi troppo simile a quella dei colleghi vostri, posso pensare che abbiate copiato. E quindi quello va estremamente a svantaggio vostro. Secondo dataset, stessi settings operativi per quanto riguarda la parte analitica, però non c'è un paper di riferimento, ma vi posso comunque dare i miei risultati, almeno come guida preliminare o come comparazione. Però il paper di una parte tecnica è riferito semplicemente all'altro studio.




Allora, oggi vi faccio vedere una presentazione, che non è una presentazione specifica per questa lezione ma che ho fatto io per una conferenza, dei miei risultati. Per il progetto, quindi, abbiamo scelto noi il dataset. Il dataset è riferito riguarda il congenital heart disease, quindi difetti cardiaci congeniti su bambini, su neonati. Non è il primo dataset pubblicato, ma è il secondo dataset, quello riferito alle donne incinte. Ora vedremo un attimo i risultati del primo dataset. Troverete, per quanto riguarda la parte sperimentale con LC-MS, potete fare il riferimento al paper, che è online. Sapete fare una ricerca bibliografica? Andate su un sito ad esempio Google Scholar, mettete parole chiave, mettete un range di data e se serve per la vostra tesi o per qualsiasi altra cosa accademica, mi raccomando, selezionate solo Peer Review Paper. Sapete che cos'è un Peer Review Paper? Com'è il processo di pubblicazione? Perché, diciamo, affidatevi solo a un Peer Review Paper?  Allora, questa è una parte importante perché, per esempio, come dicevo, Wikipedia non è attendibile perché Wikipedia non è attendibile, quantomeno non è attendibile a livello accademico, perché Wikipedia arriva persona x scrive che l'uomo non è mai stato sulla luna, rimane lì fino a che qualcuno non se ne accorge che è una castroneria e può effettuare la verifica. Il problema è che è open source, chiunque può mettere le mani sopra. Quindi non c'è un checking di verità su quello che viene scritto all'interno del sito o enciclopedia, in questo caso Wikipedia. Mentre il processo di pubblicazione di un paper funziona in questa maniera: noi scienziati e scienziate abbiamo un'idea, facciamo il nostro in modo che sia riproducibile e scriviamo il nostro paper. In base al topic, all'argomento del paper, lo mandiamo in un giornale rispetto ad un altro. I giornali hanno dei ranking che in base anche all'argomento per vedere l'importanza del giornale si valuta l'impact factor. L'impact factor è un valore che ti dice quanto quanto quel giornale è buono o no. Sostanzialmente si basa sul numero di citazioni degli articoli presenti su quel giornale. Più un articolo viene citato, più significa che l'articolo è buono ( a meno che citato in negativo). Quindi, in base all'impact factor, alla tipologia di paper, al tuo settore scientifico, perché anche in settore scientifici apre un mondo, una voragine un po' nera dell'accademia, soprattutto italiana, scegli il tuo giornale. Nel momento in cui hai scelto il tuo giornale, mandi il tuo paper all'editore. L'editore decide se il paper fitta o non fitta e lo scopo del giornale. Per esempio, alcuni giornali hanno solo review. Il review non è un esperimento, ma è una relazione, come se fosse un saggio, quindi fai la ricerca bibliografica, prendi un po' di testi, lavori su quell'argomento e fai, ... Quindi, in base alla tipologia di paper, mandi, l'editore dice se fitta o non fitta, se fitta e il giornale prevede un certo peer review, che cosa fa l'editore? Decide di mandare il vostro paper ad altri scienziati o scienziate in quel settore che controlla questo paper, controlla se non c'è scritto cose assurde, controlla se scientificamente ha senso e suggerisce delle modifiche. Quindi, i revisori, perché questo è peer review, leggono il paper e decidono se va implementato a livello scientifico, se a livello scientifico scritto è alto, se è fitta nel tipo di giornale, se l'esperimento ha senso per quel giornale, cioè se è un esperimento che fa schifo, sicuramente non pubblicherai su Nature, perché è un giornale molto alto. Poi dopo due o tre revisori controllano il paper, lo rimandano all'editore, l'editore lo rimanda agli autori, gli autori in base ai decisioni dei revisori devono effettuare delle modifiche, possono essere minor o major. Major solitamente quando deve ritornare al laboratorio per fare l'esperimento. Fai le modifiche le manda all'editore, l'editore lo rimanda ai revisori, i revisori decidono se va pubblicato così com'è o se no quesrto va da una parte all'altra fino a che come le legge in Parlamento fino a che non dicono che è perfetto, può essere pubblicato oppure no, va trasferito in un altro giornale, oppure no, non può essere pubblicato. Per questo gli diciamo controllare che gli articoli siano peer review perché significa che dei vari, cioè altri scienziati hanno stabilito che quello che c'è scritto su quell'articolo abbia senso scientifico e sia fatto col rigore scientifico. Ci sono poi giornali specifici soprattutto per la linea organica sintetica che, sintetica significa che sintetizzano le molecole, che in alcuni casi rifanno proprio l'esperimento. Cioè ci sono delle persone ..rifanno la sintesi e quello è proprio l'apice alto altissimo di pubblicazione quando qualcuno valida ulteriormente il vostro esperimento è l'apice. Come succede la revisione? Può succedere che è double blind cioè gli autori non sanno che si è revisori e i revisori non sanno chi siano gli autori  mentre solitamente adesso quello più comune è che almeno i revisori sappiano chi è che ha scritto il lavoro e chi è che ha fatto il lavoro e quello è blind semplicemente dal lato degli autori che non sanno che siano  i revisori e quindi voi nelle vostre report, nelle vostre tesi cercate solo peer review paper o anche chapter anche alcuni capitoli funzionano così su questo peer review quindi fate attenzione tipo per esempio i comunicati per esempio su nature, le lettere solitamente questi non subiscono un processo di peer review, anche le short communication solitamente non subiscono peer review quindi fate attenzione che quello che utilizzate sia materiale accreditato e questo vi serve anche per il report nostro. Poi adesso vi faccio vedere la presentazione che ho fatto io, non dico prendete il spunto, ma cercate di capire come deve essere strutturata una presentazione, so che siete familiari con le slide però ci sono alcune cose su cui noi siamo abbastanza fissate, quello che chiedo io è non avere le slide prima di tutto non devono essere troppo colorate, devono essere lineari, devono avere una struttura, devono avere un'agenda, iniziate quindi con l'indice facciamo questo questo questo devono essere strutturati con l'introduzione, materiali e metodi, risultati, conclusioni finali. Devono essere sempre leggibili quindi ci dovete mettere il numero alle slide e non è una banalità, poi devono essere chiare, devono essere chiaramente scritte in inglese, poi potete scegliere voi colore, animazione quello che più vi pare, quello che più vi rappresenta, rappresenta il progetto e chiaramente ci sono degli elementi grafici quando fate le slide: utilizzate il grassetto, utilizzate il corsivo, utilizzate queste tecniche per le parole chiave perché aiutano anche voi nella presentazione, se vi dimenticate magari la parola chiave del discorso avere le parole chiave vi può aiutare, non le fate troppo scritte, non le fate senza scritte, cercate una via di mezzo. I grafici mi raccomando: assi, label sugli assi, la leggenda per i colori, tutto deve essere leggibile se non è leggibile perché ci avete troppe variabili troppi nomi eccetera evidenziate le variabili più significativi e ce le scrivete a mano, non presentate assi senza label o senza unità di misura, mi raccomando le unità di misura. E non fate che la presentazione la fa una persona sola, fate di alternarvi e dovete sapere tutto di tutto il progetto perché non è detto che le domande avvengano solo sulla parte che state presentando. Quindi titolo, nome..qua c'è una piccola introduzione che cos'è congenital hearth disease, questo dataset come ho detto riguarda i difetti cardinaci e in realtà i difetti cardinaci è un'ampia problematica perché può riguardare diverse anomalie all'interno della diversa delle arterie e delle vene cardiache e in realtà non riguarda solo le vene ma anche le valvole, le pareti del cuore e la separazione tra atrio e ventricolo quindi capite che essendo un cappello che prende molte casistiche non è così facile identificare l'impatto biologico specifico, in realtà non sono pochi i casi purtroppo, in realtà dal punto di vista statistico è abbastanza raro perché parliamo 0,6 e 63 e 0,8 quindi sotto l'1% della popolazione però comunque ci sono sufficienti casi. Questo è un overview del paper che abbiamo già pubblicato, il primo paper ricordava l'analisi del plasma delle mamme non incinte che avevano avuto dei bambini o più bambini che presentavano un difetto cardiaco perché da studi procedenti si è evinto che in realtà è una patologia che trasmette la mamma, non è ommorica, non dipende dal padre ma dipende dalla madre per questo noi ci siamo organizzati sulla signature del plasma delle mamme e quindi abbiamo iniziato alle mamme incinte noi cosa abbiamo fatto? avevamo sostanzialmente quattro data set, avevamo neanche un descritto numero di pazienti sostanzialmente e abbiamo analizzato il plasma con lipidomica, sappiamo che dobbiamo analizzare le molecole positivo e negativo in maniera differente quindi abbiamo lipidomica positiva e lipidomica negativa e abbiamo metabolomica positiva e metabolimica negativa e le abbiamo analizzate come single block cioè quindi ho fatto dei modelli di classificazione indipendentemente su ogni blocco, vedremo le lezioni successive di cosa si intende per blocco e cosa si intende per fusion però quello che cosa faccio, ho accorpato sostanzialmente i blocchi di positivo e negativo di lipodomica e i blocchi di metabolomica positivo e negativo in modo da ottenere un unico data set di lipidomica e un unico data set di metabolomica e un data set che fondeva lipidomica e meatbolomica, non è banale come cosa perché vedete che il numero di variabili aumenta tantissimo e ottenevamo dei risultati anche in validazione molto promettenti soprattutto dal punto di vista dell'utilizzo della metabolomica, successivamente ho applicato un modello specifico middle level fusion che si fa una selezione di variabili a priori ottenendo così delle performance per quanto riguarda l'accuratezza all'incirca il 94% nella situazione più ottimale utilizzando sostanzialmente un quantitativo di molecole molto basso perché qui si parla di, facendo una selezione di variabili a priori di meno di 15 variabili per combinazione ottenevamo un'accuratezza di circa poco meno del 95% sul test. Anche nella classificazione di un secondo caso che era una patologia specifica del CHD si ottenevano dei risultati promettenti e questo era sulle donne dopo la gravidanza, questo era il primo dataset. Ovviamente dovevamo dire che quelle 15 variabili che avevamo selezionato potessero dipendere da BMI e GSEX e invece poi quelle variabili significative le abbiamo studiate rispetto ai parametri demografici e abbiamo visto che in realtà non dipendevo dai parametri demografici che abbiamo registrato perché chiaramente in questo caso non è che noi possiamo fare troppo schizzinosi e dire ok io mi prendo solo le donne che hanno una sola gravidanza, mi prendo le donne dell'età dai 20 ai 35 anni, mi prendo le donne non fumatrice eccetera in questo caso la campagna di raccolta dei campioni viene fatta in maniera non supervisionale nel senso quello che viene ti prendi perché stiamo parlando comunque di numeri molto bassi e in questo caso era in collaborazione con l'ospedale dell'università di Bristol quindi la casistica della patologia dipendeva da quello che raccoglievano in ospedale mentre per quanto riguarda esami era una corte di una databank. Ora torniamo a noi, qual è il nostro scopo? identificare in questo nuovo dataset dove la campagna è effettuata su donne incinte prima se c'è una differenza tra donne incinte con bambini che poi presentano malformazioni cardiache rispetto a donne con healthy children e poi eventualmente identificare quelli che possono essere successivamente valutati e validati come biomarcatori per tale patologia, il concetto di biomarcatori all'interno della medicina e della biologia è un concetto molto molto molto complesso quindi non chiamare quelle molecole che troverete significative biomarcatori, al massimo sono potenziali biomarcatori perché i nostri metodi hanno un limite, i nostri dataset sono ottenuti con LC-MS untargeted, quindi non è una quantitativa quando inizio a parlare del biomarcatore, innanzitutto devono essere valori quantitativi secondo devono essere valori che vengono validati su corti di centinaia e centinaia di pazienti e devono essere campagne fatte all'interno quindi non basta che un dataset di 800 pazienti che inizialmente...e presenta queste molecole significative. Quindi noi che cosa abbiamo fatto? donne incinte raccolto il plasma e abbiamo studiato il plasma delle donne incinte. Di che cosa si compone il vostro dataset? avete 78 casi e 100 pazienti, vedrete in realtà quando adesso inizierete a importare avete numeri più alti di questi, vedrete nel nome del file un underscore 00 e un underscore 01 perché alcuni di alcuni campioni è stato preso un duplicato tecnico cioè è stato analizzato due volte per fare una valutazione di quella che sia la riproducibilità tecnica strumentale, se 01 ha lo stesso nome del precedente solo che cambia 00 perché è un duplicato tecnico vuol dire che lo stesso campione è stato analizzato due volte quindi vedremo in questo caso i duplicati tecnici vengono rimossi cioè rimuovete il secondo campione perché se avesse avuto un duplicato tecnico di tutti i campioni potevate fare la media siccome invece voi ce l'avete solo di alcuni la media non è il caso di farla perché altrimenti alcuni campioni avrebbero un'informazione con una robustezza differente di altri e quindi è il caso di eliminarlo, quindi abbiamo questa corte di pazienti, abbiamo estratto il plasma che poi è stato analizzato in maniera indipendente tra lipidomica e metabolomica, voi avete solo i dati di metabolomica perché sono quelli che sono venuti meglio. Che cosa ho fatto io nel mio piccolo e che cosa vi presento oggi? questo è chiaramente il data set è stato già analizzato, i risultati ce li abbiamo già, teoricamente è stato scritto anche il paper ma è in fase di rifinitura Noi ne abbiamo 100 pazienti sani 78 casi, in maniera esplorativa ho fatto l'analisi delle componenti principali su i singoli blocchi quindi nel mio caso avevo 4 blocchi: .. lipidomica positivo, negativo, metabolomica positivo negativo,poi ho fatto la fusione dei dati di metabolomica e lipidomica utilizzando un algoritmo che si chiama CONDIM, probabilmente lo vedremo successivamente quando faremo la fusione dei dati, su questo ho applicato un algoritmo per la selezione del training e del test set che si chiama duplex in maniera tale di avere un training e un test set comune per tutti i blocchi perchè questa è la parte importante, se voi fate...non è detto fare la fusione dei dati, però se volete comparare le performance ottenuti da due dataset chiaramente il dataset dei training e di test set devono essere gli stessi non è che su uno ci mettete alcuni pazienti e sul lato ce ne mettete meno a partire dagli stessi pazienti, noi metabolomica positiva e metabolomica negativa sono gli stessi pazienti, se io voglio fare la comparazione se performa meglio il positivo o il negativo chiaramente il modello che devo costruire è in maniera tale che siano comparabili quindi ci siano all'interno del training gli stessi campioni per entrambi i dataset e non è che si può scegliere il training rispetto a uno o rispetto all'altro, lo scegliete in maniera che sia bilanciato l'optimum per tutte e due, poi ho provato delle tecniche di classificazione, una banale PLS-DA che ancora non abbiamo visto, è una tecnica di classificazione supervisionata molto simile alla PCA, si parte da una decomposizione nello spazio e poi si .. essenzialmente una classificazione per LDA, poi c'è un low-level fusion PLS-DA e poi come dicevo prima stecoc stender PLSDA che consente di selezionare le variabili. Tutto questo è stato validato in cross-validation perché i campioni sono pochi quindi non hanno potuto dividere il test in training, validazione e test ma solo in training e test e poi riportato e poi validato tramite il Nation Plan. Come vedete nelle slide dei materiali e metodi devono essere esaustivo, ci deve essere il pre-processamento che avete utilizzato, ci deve essere come è composto il dataset, ci devono essere tutti i parametri che avete messo e che avete ottimizzato all'interno dei vostri modelli perché dalla slide dei materiali meriti si deve capire se c'è qualche errore anche formale nella parte di analisi e deve essere ripetibile. Poi se notate sopra potete seguire a che punto siamo nella presentazione. Quindi noi partiamo da questi, nel vostro caso partite da questi due dataset. Ovviamente non dovete fare lo stesso mio workflow. Quindi voi avrete 98 variabili e 52 variabili, neanche troppe, d dati sono stati già allineati, le variabili sono state già filtrate, e sono stati identificate. Quindi partire da un dataset già pulito, perché altrimenti sarebbe stato troppo difficile. Quindi lavorerete già con un dataset abbastanza preprocessato e lavorato. Quindi tutta la parte di smoothing, allineamento, eccetera, quella parte lì non la dovete fare. Però dovete studiarla per la teoria. Quindi la mia divisione era così. Chiaramente la divisione di training e test set io  l'ho fatta indipendente per i controlli e indipendente per i casi, in maniera tale che si fossero ottimizzati e poi i metodi per selezionare training e test set. Poi, siccome chiaramente c'era il problema di, avevamo un problema sulla parte di classificazione, siccome io sono molto scettica su alcune tecniche tipo Random Forrest, che è troppo ottimistico, tecniche che chiaramente non sono fatte per questa tipologia di dati, o magari anche tecniche più semplici che però per questa tecnologia di dati diciamo non performano benissimo perché, come per esempio LDA, support vector machine, perché entrambe si basano sulla dimensionalità del dataset e sulle distanze, quindi potrebbe essere difficile la classificazione. Ramon Forrest non mi piace perché è randomico e ottimistico, e questi due chiaramente..  E quindi però, nonostante io sia scettica, ho paragonato il mio modello ottimale con questi modelli che in realtà vanno tanto di moda. E poi mi sono posto un problema. Ho comunque troppi variabili, e quindi ho detto proviamo magari questi modelli performano male perché ci sono troppi variabili, e quindi proviamo ad applicare degli approcci che possono essere di variabile reduction tipo MRMR, oppure di Space Reduction come la PCA, oppure l'applicazione di MRMR e PCA insieme per vedere se riducendo prima il numero di variabili e poi massimizzando l'informazione questi modelli potessero migliorare. Poi l'altro approccio che ho approvato è la data augumentation che a voi piace tanto, a voi come categoria. Io ovviamente mi sono focalizzata sulla classe minoritaria e ho cercato di proiettare nuovi campioni rispetto alla classe minoritaria utilizzando diversi approcci. Quindi con diversi livelli di complessità e diverse caratteristiche rispetto, cioè tenendo in considerazione la classe maggioritaria, non tenendo in considerazione la classe maggioritaria e via dicendo. Questi sono i cromatogrammi che vanno, vabbe voi non avete gli spettri, dovete sempre, sempre plottare i dati. O li plottate così, quindi a mezz'aria rispetto all'intensità o li plottata, non so come fate la mappa o fate la PCA ma ad ogni step, ad ogni preprocessamento che voi fate sui dati dovete graficare per verificare se avete fatto qualche cosa che fa danno. Queste sono le mie prime PCA sui singoli blocchi vedete che la differenziazione per quanto riguarda la lipidomica non c'è, qualcosa si inizia a vedere per quanto riguarda la metabolomica e soprattutto la metabolomica ..positiva..., dove si vede lungo PC1 la separazione. Ora, guardate attentamente queste PCA: scoreplot, loadingplot, sugli assi dove si esprime è riportata, non si vede bene, c'è riportata anche l'explained variance. I dati, i grafici devono essere spiegati. Loadingplot, m0 rispetto all'intensità dei loadings e leggenda, titolo..ecco cosa si intende per i grafici che si spiegano da soli e titolo generale, quello che si capisca di che cosa stiamo parlando. Non fate PCA, rappresentare gli score senza loadings, non fate PCA dove non ci stanno l'explained variance per ogni asse anche se tanto il vostro pacchetto libreria lo fa in automatico l'explained variance, però mi raccomando. Poi questo è il risultato della fusione totale di tutti e quattro i blocchi però questa è una situazione un po' più complessa...e questi sono il risultati miei di classificazione quindi vedete che in cross-validation comunque si ottengono dei buoni risultati per quanto riguarda la metabolomica in positivo, carichiamo circa il 93%, sul test chiaramente si abbassa e scende al 77%, quando andiamo a fare la fusione teniamo in considerazione tutte e due i metabolomi che si arriva sul test del 77%, poi questo in realtà è perché la tecnica che faccio io di sPLS-DA, praticamente dipende dall'ordine con cui combinate i blocchi quindi siccome dipende dall'ordine di combinazione, noi abbiamo quattro blocchi, io ho fatto tutte le combinazioni quindi sono ci sono 64 modelli per valutare qual è l'accuratezza e come vedete più o meno le accuratezze ritornano sono in validazione intorno al 96-97% con un numero minimo di variabili, otteniamo un ottimo risultato utilizzando tutti e quattro i blocchi dove abbiamo un accuratezza del test set all'84% sostanzialmente utilizzando solo tre variabili, tre molecole: una dalla lipidomica positiva e due dalla metabolomica positiva, questo è poi l'approccio. Quando io dico che è un ottimo risultato 84%, secondo voi fa schifo? no ma perché in questo caso specifico l'84% non è basso? chi è che abbiamo analizzato? a chi abbiamo prelevato il sangue? alle madri incinte, immaginate lo sbalzo ormonale i cambio metabolico che avviene all'interno di una donna incinta, qui hanno provato a prenderlo allo stesso tempo il sangue della gravidanza però già vi portate indietro una variabilità biologica molto alta, quando poi parlate degli studi sulle donne tutto diventa ancora più complesso perché noi abbiamo dei cicli, delle variazioni ormonali molto più alte rispetto ai uomini, quando una donna è incinta questo diventa ancora più complesso quindi stiamo parlando di una variabilità biologica assurda e l'84% in questo caso è un ottimo risultato. Soprattutto stiamo parlando di una patologia che non è poi così chiara, vi dico solo che stiamo facendo adesso uno studio su praticamente l'epatocarcinoma, stiamo cercando di classificare diversi stadi della malattia, ho delle accuratezze del 20%. Come vedete io riporto i modelli ottimali per ogni cosa, non sono propri i modelli ottimali da tutte le combinazioni di quello che vi ho presentato prima ma se questi sono i risultati ottimali il mio modello batte tutte le qualsiasi combinazioni ottenute e anche il fatto che abbia provato la data augmentation non ha portato grandi risultati innanzitutto perché i dati sintetici hanno lasciato un po' di tempo che trovano, secondo perché chiaramente quelli si basano su una variabilità che hanno già visto quindi il bilanciamento del dataset non ha portato nessun beneficio questo non significa che voi non dobbiate provare. Ok, questo è quello che ho fatto io, chiaramente un po' di scetticismo...adesso stiamo passando sull'untargeted.. E questa era la mia presentazione come vedete ci stanno le conclusioni e l'approccio di future work quindi il future work, la sezione future work potrebbe essere una critica del vostro sistema e di cose che potreste fare, io apprezzo molto la parte del future work perché significa essere concreti. Inanzitutto scaricate i due dataset, importate i dataset in python. Se avete scaricato il dataset vedrete che avete i pazienti sulle colonne, le variabili sulle righe quindi tenete questo in considerazione, poi c'è la prima riga è il nome del campione, la seconda riga è la classe, voi avete due aspetti che dovete tenere in considerazione: di alcuni avete il duplicato tecnico quindi 00 e 01, poi l'altro aspetto che dovete tenere in considerazione è la presenza dei QC,   e ricordate che abbiamo detto sui QC e la stessa cosa vale anche per i duplicati? A che servono i quality control..come sono ottenuti e a cosa servono? sono ottenuti o unendo tutti i campioni, una parte di tutti o decido di fare un subset e unisco solo il subset quindi quando fate la PCA la caratteristica che devono avere i QC è che devono essere tutti vicini tra di loro perché quando voi trovate QC QC QC QC quello è lo stesso campione, la stessa madre dove ci sono tutti i pool di campioni che è stato diviso in piccole .. ed è stata analizzato all'interno della corsa in vari tempi quindi essendo sempre lo stesso campione analizzato diverse volte deve clusterizzare molto all'interno per esempio della vostra PCA. Se questo non succede può avvenire per due motivi, uno  che lo strumento non risponde bene nel tempo all'analisi, la seconda possibilità è che c'è stata la degradazione del QC, cosa che può avvenire. Può avvenire se non si prendono delle accortezze nella parte pratica e poi c'è il duplicato tecnico, il duplicato tecnico inizialmente vi conviene tenerlo nel dataset perché dovete fare prima un'assessment sulla qualità del dato e la qualità dell'analisi, questa cosa come la fate? sia valutando i QC sia valutando il duplicato tecnico perché il duplicato tecnico devono essere molto vicini tra di loro perché sostanzialmente sto analizzando lo stesso campione due volte quindi nella prima fase che è nella fase di vedere come ha performato l'analisi, QC e duplicati tecnici li lasciate, nella seconda fase iniziate a provare tecniche di normalizzazione, tecniche di preprocessamento e via dicendo togliete i QC e togliete i duplicati tecnici, siccome poi il duplicato tecnico non ce l'avete per tutti i campioni ma ce l'avete solo per alcuni perché sennò avrebbero chiesto troppo tempo non dovete fare la media, solitamente se l'analisi è stata fatta duplicato duplicato tecnico quello che viene iniziato a fare è mediare tutti duplicati cioè stesso campione analizzato più volte, fate la media e prendete la media, in questo caso non si può fare sennò si direbbe che alcuni campioni li avete mediati e alcuni campioni no quindi la robustezza del dato sarebbe diversa quindi vi suggerisco di rimuovere il secondo duplicato. Ricordate che intanto dovete fare i riprocessamenti..dovete testare diverse normalizzazioni, perché i dati sono stati filtrati, annotati ,a non sono normalizzati, quindi la parte di normalizzazione chimica (e non fate l'autoscaling, ricordate la differenza tra normalizzazione chimica e scalaggio).